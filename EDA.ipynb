{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import torch, copy, json, requests, os, time, cv2, glob, random, shutil\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65179</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>[{'original_width': 1382, 'original_height': 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66738</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>[{'original_width': 1440, 'original_height': 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65282</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>[{'original_width': 1440, 'original_height': 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66760</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>[{'original_width': 1920, 'original_height': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65876</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>[{'original_width': 1194, 'original_height': 7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               data  \\\n",
       "0  65179  {'image': 'https://locofy-ai-task-production.s...   \n",
       "1  66738  {'image': 'https://locofy-ai-task-production.s...   \n",
       "2  65282  {'image': 'https://locofy-ai-task-production.s...   \n",
       "3  66760  {'image': 'https://locofy-ai-task-production.s...   \n",
       "4  65876  {'image': 'https://locofy-ai-task-production.s...   \n",
       "\n",
       "                                         annotations  \n",
       "0  [{'original_width': 1382, 'original_height': 4...  \n",
       "1  [{'original_width': 1440, 'original_height': 6...  \n",
       "2  [{'original_width': 1440, 'original_height': 5...  \n",
       "3  [{'original_width': 1920, 'original_height': 1...  \n",
       "4  [{'original_width': 1194, 'original_height': 7...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_path = './data/images/'\n",
    "f = open('./data/Locofy_data.json')\n",
    "data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['grid', 'popup', 'progress_bar', 'none', 'raw']:\n",
    "    create_path = base_path + cat \n",
    "    if not os.path.exists(create_path): os.mkdir(create_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1955/1955 [00:00<00:00, 28549.77it/s]\n"
     ]
    }
   ],
   "source": [
    "failed_batch = []\n",
    "for num in tqdm(range(df.shape[0])):\n",
    "    try:\n",
    "        id, image_path, annot_ls = df.loc[num].values\n",
    "        if not os.path.isfile(base_path + f'raw/{str(id)}.png'):\n",
    "            res = requests.get(image_path['image'])\n",
    "            img = Image.open(BytesIO(res.content))\n",
    "            img.save(base_path + f'raw/{str(id)}.png')\n",
    "    except:\n",
    "        failed_batch.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['id'].isin(failed_batch)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(input_dict):\n",
    "    if pd.isna(input_dict): return (np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan)\n",
    "    else: \n",
    "        o_w, o_h = input_dict['original_width'], input_dict['original_height']\n",
    "        x, y = input_dict['value']['x']/100, input_dict['value']['y']/100\n",
    "        w, h = input_dict['value']['width']/100, input_dict['value']['height']/100\n",
    "        label = input_dict['value']['label']\n",
    "        return (x * o_w, y * o_h, w * o_w , h * o_h , label, o_w, o_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['x', 'y', 'width', 'height', 'label', 'img_width', 'img_height' ]] = df['annotations'].map(extract_info).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['img_path'] = df['data'].apply(lambda x : x['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "NaN             1274\n",
       "grid             737\n",
       "progress bar      99\n",
       "slider            29\n",
       "pop-up            21\n",
       "google maps        2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>data</th>\n",
       "      <th>annotations</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>label</th>\n",
       "      <th>img_width</th>\n",
       "      <th>img_height</th>\n",
       "      <th>img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65179</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>{'original_width': 1382, 'original_height': 41...</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>1704.000000</td>\n",
       "      <td>977.000000</td>\n",
       "      <td>623.000000</td>\n",
       "      <td>grid</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>4189.0</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66738</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>{'original_width': 1440, 'original_height': 67...</td>\n",
       "      <td>55.923832</td>\n",
       "      <td>1184.000000</td>\n",
       "      <td>1325.266588</td>\n",
       "      <td>698.177150</td>\n",
       "      <td>grid</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6712.0</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66738</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>{'original_width': 1440, 'original_height': 67...</td>\n",
       "      <td>62.677191</td>\n",
       "      <td>2752.721437</td>\n",
       "      <td>1316.221013</td>\n",
       "      <td>425.454396</td>\n",
       "      <td>grid</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6712.0</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66738</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>{'original_width': 1440, 'original_height': 67...</td>\n",
       "      <td>60.655346</td>\n",
       "      <td>5497.915278</td>\n",
       "      <td>1328.352083</td>\n",
       "      <td>593.610181</td>\n",
       "      <td>grid</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>6712.0</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65282</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>{'original_width': 1440, 'original_height': 55...</td>\n",
       "      <td>728.000000</td>\n",
       "      <td>2193.994550</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>525.005450</td>\n",
       "      <td>grid</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>5501.0</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>75413</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>75638</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>76495</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2160</th>\n",
       "      <td>75247</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2161</th>\n",
       "      <td>75958</td>\n",
       "      <td>{'image': 'https://locofy-ai-task-production.s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://locofy-ai-task-production.s3.ap-southe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2162 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               data  \\\n",
       "0     65179  {'image': 'https://locofy-ai-task-production.s...   \n",
       "1     66738  {'image': 'https://locofy-ai-task-production.s...   \n",
       "2     66738  {'image': 'https://locofy-ai-task-production.s...   \n",
       "3     66738  {'image': 'https://locofy-ai-task-production.s...   \n",
       "4     65282  {'image': 'https://locofy-ai-task-production.s...   \n",
       "...     ...                                                ...   \n",
       "2157  75413  {'image': 'https://locofy-ai-task-production.s...   \n",
       "2158  75638  {'image': 'https://locofy-ai-task-production.s...   \n",
       "2159  76495  {'image': 'https://locofy-ai-task-production.s...   \n",
       "2160  75247  {'image': 'https://locofy-ai-task-production.s...   \n",
       "2161  75958  {'image': 'https://locofy-ai-task-production.s...   \n",
       "\n",
       "                                            annotations           x  \\\n",
       "0     {'original_width': 1382, 'original_height': 41...  195.000000   \n",
       "1     {'original_width': 1440, 'original_height': 67...   55.923832   \n",
       "2     {'original_width': 1440, 'original_height': 67...   62.677191   \n",
       "3     {'original_width': 1440, 'original_height': 67...   60.655346   \n",
       "4     {'original_width': 1440, 'original_height': 55...  728.000000   \n",
       "...                                                 ...         ...   \n",
       "2157                                                NaN         NaN   \n",
       "2158                                                NaN         NaN   \n",
       "2159                                                NaN         NaN   \n",
       "2160                                                NaN         NaN   \n",
       "2161                                                NaN         NaN   \n",
       "\n",
       "                y        width      height label  img_width  img_height  \\\n",
       "0     1704.000000   977.000000  623.000000  grid     1382.0      4189.0   \n",
       "1     1184.000000  1325.266588  698.177150  grid     1440.0      6712.0   \n",
       "2     2752.721437  1316.221013  425.454396  grid     1440.0      6712.0   \n",
       "3     5497.915278  1328.352083  593.610181  grid     1440.0      6712.0   \n",
       "4     2193.994550   620.000000  525.005450  grid     1440.0      5501.0   \n",
       "...           ...          ...         ...   ...        ...         ...   \n",
       "2157          NaN          NaN         NaN   NaN        NaN         NaN   \n",
       "2158          NaN          NaN         NaN   NaN        NaN         NaN   \n",
       "2159          NaN          NaN         NaN   NaN        NaN         NaN   \n",
       "2160          NaN          NaN         NaN   NaN        NaN         NaN   \n",
       "2161          NaN          NaN         NaN   NaN        NaN         NaN   \n",
       "\n",
       "                                               img_path  \n",
       "0     https://locofy-ai-task-production.s3.ap-southe...  \n",
       "1     https://locofy-ai-task-production.s3.ap-southe...  \n",
       "2     https://locofy-ai-task-production.s3.ap-southe...  \n",
       "3     https://locofy-ai-task-production.s3.ap-southe...  \n",
       "4     https://locofy-ai-task-production.s3.ap-southe...  \n",
       "...                                                 ...  \n",
       "2157  https://locofy-ai-task-production.s3.ap-southe...  \n",
       "2158  https://locofy-ai-task-production.s3.ap-southe...  \n",
       "2159  https://locofy-ai-task-production.s3.ap-southe...  \n",
       "2160  https://locofy-ai-task-production.s3.ap-southe...  \n",
       "2161  https://locofy-ai-task-production.s3.ap-southe...  \n",
       "\n",
       "[2162 rows x 11 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create None Class\n",
    "1. use adaptive thresholding binary inverse thresholding and find a suitable area of interest by k nearest neighbour to group different features together\n",
    "2. None class create sufficient images to compare against slider vs progress bar and popup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_annotations = df[df['annotations'].isna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for id in df_na_annotations['id'][-100:-20].tolist():\n",
    "    img = cv2.cvtColor(cv2.imread(f'./data/images/raw/{str(id)}.png'), cv2.COLOR_BGR2GRAY)\n",
    "    mean_value = img.mean()\n",
    "    blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, blockSize=11, C=5)\n",
    "    (T, threshInv) = cv2.threshold(blurred, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)\n",
    "    _, binary_image = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "    print('binary inverse', T, _)\n",
    "    \n",
    "    fig, ax = plt.subplots(1,4, figsize=(15,5))\n",
    "    ax[0].imshow(img,  cmap='gray', vmin=0, vmax=255)\n",
    "    ax[1].imshow(threshInv,  cmap='gray', vmin=0, vmax=255)\n",
    "    ax[2].imshow(adaptive_thresh,  cmap='gray', vmin=0, vmax=255)\n",
    "    ax[3].imshow(binary_image,  cmap='gray', vmin=0, vmax=255)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na_annotations.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 100/100 [00:24<00:00,  4.11it/s]\n"
     ]
    }
   ],
   "source": [
    "for id in tqdm(df_na_annotations['id'].tolist()[200:300]):\n",
    "    img = cv2.imread(f'./data/images/raw/{str(id)}.png')\n",
    "    grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(grey, (3, 3), 0)\n",
    "    adaptive_thresh = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, blockSize=11, C=5)\n",
    "    contours, _ = cv2.findContours(adaptive_thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    counter = 0\n",
    "    while len(bounding_boxes) > 100 and counter < 3:\n",
    "        \n",
    "        box_centers = np.array([(x + w // 2, y + h // 2) for (x, y, w, h) in bounding_boxes])\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=10, radius=200).fit(box_centers)\n",
    "        distances, indices = nbrs.kneighbors(box_centers)\n",
    "        \n",
    "        merged_boxes = []\n",
    "        distance_threshold = 500 \n",
    "        for i, (x, y, w, h) in enumerate(bounding_boxes):\n",
    "            if distances[i][1] < distance_threshold:\n",
    "                neighbor_idx = indices[i][1]\n",
    "                x2, y2, w2, h2 = bounding_boxes[neighbor_idx]\n",
    "                \n",
    "                new_x = min(x, x2)\n",
    "                new_y = min(y, y2)\n",
    "                new_w = max(x + w, x2 + w2) - new_x\n",
    "                new_h = max(y + h, y2 + h2) - new_y\n",
    "                merged_boxes.append((new_x, new_y, new_w, new_h))\n",
    "            else:\n",
    "                merged_boxes.append((x, y, w, h))\n",
    "        \n",
    "        bounding_boxes = non_max_suppression(merged_boxes, 0.1)\n",
    "        counter += 1\n",
    "    final_boxes = sorted(bounding_boxes, key=lambda box: box[2] * box[3], reverse=True)  # Sort by area in descending order\n",
    "    for (count, (x, y, w, h)) in enumerate(final_boxes[:5]):\n",
    "        cv2.imwrite(f'./data/images/none/largest_cut_v2_{str(id)}_{str(count)}.png', img[y:y+h, x:x+w])\n",
    "    # print()\n",
    "    # print(f'{id=}')\n",
    "    # for (x, y, w, h) in final_boxes[:3]:\n",
    "    #     print(x, y, w, h)\n",
    "    #     fig, ax = plt.subplots(1,3, figsize=(15,5))\n",
    "    #     ax[0].imshow(img,  cmap='gray', vmin=0, vmax=255)\n",
    "    #     ax[1].imshow(adaptive_thresh,  cmap='gray', vmin=0, vmax=255)\n",
    "    #     ax[2].imshow(img[y:y+h, x:x+w], cmap='gray', vmin=0, vmax=255)\n",
    "    #     plt.tight_layout()\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### popup class \n",
    "1. rotation will be randomly done during transform.compose\n",
    "2. consider increase number of images by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "popup_ls = glob.glob('./data/images/popup/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def adjust_hue(image, delta=0.1):\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    hsv_image = np.float32(hsv_image)\n",
    "    hsv_image[:, :, 0] = (hsv_image[:, :, 0] + delta * 180) % 180\n",
    "    hsv_image = np.clip(hsv_image, 0, 255).astype(np.uint8)\n",
    "    return cv2.cvtColor(hsv_image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "def random_rgb_swap(image):\n",
    "    r,g,b = cv2.split(image)\n",
    "    channels =[r,g,b]\n",
    "    random.shuffle(channels)  # Shuffle the channels randomly\n",
    "    swapped_image = cv2.merge(channels)\n",
    "    return swapped_image\n",
    "    \n",
    "def adjust_contrast(image, factor=1.5):\n",
    "    adjusted = np.clip(image * factor, 0, 255).astype(np.uint8)\n",
    "    return adjusted\n",
    "\n",
    "def random_cutout(image, max_fraction=0.3):\n",
    "    img_height, img_width = image.shape[:2]\n",
    "\n",
    "    max_area = int(img_height * img_width * max_fraction)\n",
    "    cutout_area = random.randint(1, max_area)\n",
    "    for _ in range(30):\n",
    "        cutout_height = random.randint(1, img_height)\n",
    "        cutout_width = cutout_area // cutout_height\n",
    "        if cutout_width <= img_width:\n",
    "            break\n",
    "        else:\n",
    "            cutout_height, cutout_width = 1, 1\n",
    "\n",
    "    cutout_x = random.randint(0, img_width - cutout_width)\n",
    "    cutout_y = random.randint(0, img_height - cutout_height)\n",
    "\n",
    "    image[cutout_y:cutout_y + cutout_height, cutout_x:cutout_x + cutout_width] = 0\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/images/popup/popup_4_49720.png'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popup_ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'49720'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = popup_ls[0].split('_')[-1].split('.')[0]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in popup_ls:\n",
    "    id = path.split('_')[-1].split('.')[0]\n",
    "    img = cv2.imread(path)\n",
    "    for i in range(3):\n",
    "        img = random_rgb_swap(img)\n",
    "        img = adjust_contrast(img, factor=10)\n",
    "        img = adjust_hue(img)\n",
    "        img = random_cutout(img)\n",
    "        cv2.imwrite(f'./data/images/popup/data_augment_{str(i)}_{id}.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar_ls = glob.glob('./data/images/progress_bar/*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in progress_bar_ls:\n",
    "    id = path.split('_')[-1].split('.')[0]\n",
    "    img = cv2.imread(path)\n",
    "    for i in range(3):\n",
    "        img = random_rgb_swap(img)\n",
    "        img = adjust_contrast(img, factor=2)\n",
    "        img = adjust_hue(img)\n",
    "        cv2.imwrite(f'./data/images/progress_bar/data_augment_{id}_{str(i)}.png', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slider bar within None Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "slider_ls = glob.glob('./data/images/none/none*.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in slider_ls:\n",
    "    id = path.split('_')[-1].split('.')[0]\n",
    "    img = cv2.imread(path)\n",
    "    for i in range(3):\n",
    "        img = random_rgb_swap(img)\n",
    "        img = adjust_contrast(img, factor=2)\n",
    "        img = adjust_hue(img)\n",
    "        cv2.imwrite(f'./data/images/none/data_augment_{id}_{str(i)}.png', img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
